# -*- coding: utf-8 -*-
"""BERT_finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EYF9vsE4RFVxn_s_Tlaavvx97k9iSGpM
"""

import random
import numpy as np
import torch
#from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available
from transformers import DataCollatorWithPadding, BertConfig, BertForSequenceClassification, TrainingArguments, Trainer, BertTokenizerFast
from transformers import BertTokenizerFast
import os
from datasets import load_dataset
from sklearn.metrics import accuracy_score
from argparse import ArgumentParser


"""
def set_seed(seed: int):
    #Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if
    #installed).

    #Args:
    #    seed (:obj:`int`): The seed to set.
    random.seed(seed)
    np.random.seed(seed)
    if is_torch_available():
        torch.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        # ^^ safe to call this function even if cuda is not available
    if is_tf_available():
        import tensorflow as tf

        tf.random.set_seed(seed)

set_seed(1)
"""

def main():
	
	parser = ArgumentParser()
	parser.add_argument('--input', '-i', default=None, type=str, required=True, help="Input CSV file")	
	parser.add_argument('--model_path', '-mp', default=None, type=str, required=True, help="Pretrained model path")		    
	parser.add_argument('--maxlength', '-m', default=1536, type=int, help="Max sequence length for the model")
	parser.add_argument('--epochs', '-e', default=10, type=int, help="Number of epochs during the training")
	parser.add_argument('--per_train_batch', '-pt', default=None, type=int, required=True, help="Per device train batch size")
	parser.add_argument('--gradient_accumulation_step', '-g', default=None, type=int, required=True, help="Accumulating the gradients before updating the weights")
	parser.add_argument('--per_eval_batch', '-pe', default=None, type=int, required=True, help="Per device eval batch size")
	parser.add_argument('--logging_steps', '-l', default=1000, type=int, help="Evaluate, log and save model checkpoints every chosen step")
	parser.add_argument('--save_steps', '-s', default=1000, type=int, help="Evaluate, log and save model checkpoints every chosen step")
	parser.add_argument('--weight_decay', '-wd', default=0.01, type=float, help="strength of weight decay")
	parser.add_argument('--warmup_step', '-ws', default=500, type=int, help="number of warmup steps for learning rate scheduler")		
	args = parser.parse_args()
	

	# load the model checkpoint
	model = BertForSequenceClassification.from_pretrained(f"{args.model_path}", num_labels=2).to("cuda")
	# load the tokenizer
	tokenizer = BertTokenizerFast.from_pretrained("pretrained_bert", do_lower_case=True)

	max_length = args.maxlength

	dataset = load_dataset('csv', data_files=f'{args.input}', column_names=['text', 'label'] ,split='train')
	dataset = dataset.shuffle()
	d = dataset.train_test_split(test_size=0.1)

	# tokenize the dataset, truncate when passed `max_length`, 
	# and pad with 0's when less than `max_length`
	train_encodings = tokenizer(d["train"]["text"], truncation=True, padding=True, max_length=max_length)
	valid_encodings = tokenizer(d["test"]["text"], truncation=True, padding=True, max_length=max_length)

	class NewsGroupsDataset(torch.utils.data.Dataset):
	    def __init__(self, encodings, labels):
    		self.encodings = encodings
    		self.labels = labels

	    def __getitem__(self, idx):
    		item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}
    		item["label"] = torch.tensor(self.labels[idx])
    		return item

	    def __len__(self):
    		return len(self.labels)

	# convert our tokenized data into a torch Dataset
	train_dataset = NewsGroupsDataset(train_encodings, d["train"]["label"])
	valid_dataset = NewsGroupsDataset(valid_encodings, d["test"]["label"])

	def compute_metrics(pred):
	  labels = pred.label_ids
	  preds = pred.predictions.argmax(-1)
	  # calculate accuracy using sklearn's function
	  acc = accuracy_score(labels, preds)
	  return {
	      'accuracy': acc,
	  }

	training_args = TrainingArguments(
	    output_dir='./finetuned_bert',          # output directory
	    num_train_epochs=args.epochs,              # total number of training epochs
	    per_device_train_batch_size=args.per_train_batch,  # batch size per device during training
	    gradient_accumulation_steps=args.gradient_accumulation_step,  # accumulating the gradients before updating the weights
	    per_device_eval_batch_size=args.per_eval_batch,   # batch size for evaluation
	    warmup_steps=args.warmup_step,                # number of warmup steps for learning rate scheduler
	    weight_decay=args.weight_decay,               # strength of weight decay
	    logging_dir='./logs',            # directory for storing logs
	    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)
	    metric_for_best_model="accuracy",# but you can specify `metric_for_best_model` argument to change to accuracy or other metric
	    logging_steps=args.logging_steps,               # log & save weights each logging_steps
	    save_steps=args.save_steps,
	    evaluation_strategy="steps",     # evaluate each `logging_steps`
	    save_total_limit=3,           # whether you don't have much space so you let only 3 model weights saved in the disk
	)

	trainer = Trainer(
	    model=model,                         # the instantiated Transformers model to be trained
	    args=training_args,                  # training arguments, defined above
	    train_dataset=train_dataset,         # training dataset
	    eval_dataset=valid_dataset,          # evaluation dataset
	    compute_metrics=compute_metrics,     # the callback that computes metrics of interest
	)

	# train the model
	trainer.train()
	
if __name__ == "__main__":
    main()
